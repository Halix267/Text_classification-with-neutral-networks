{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon Review with Neural networks(CNN).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzM1sLCi1BtkLzZmsZ9Mnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Halix267/Text_classification-with-neutral-networks/blob/master/Amazon_Review_with_Neural_networks(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dag05S2HdXkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8d10409-41b0-4a07-9c9f-8e873e08c0e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXegn4-Pv6_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip         # downloading the pretrained embedding matrix of about 6 billion vocablary\n",
        "!unzip '/content/gdrive/My Drive/Amazon review/glove.6B.zip.1'   # unzipping the zip file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbXEUosUASHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd_xcMDlAr2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the dataset\n",
        "dataset=pd.read_json('/content/gdrive/My Drive/Amazon review/Amazon_Instant_Video_5.json',lines=True)\n",
        "Final_data=pd.DataFrame(dataset,columns=['summary','overall'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN3a97KGBEB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting the dataset into training and text set\n",
        "data=Final_data['summary'].values      #Data\n",
        "\n",
        "#Mapping the reviews into positive,negative and neutral\n",
        "sentiment = []\n",
        "for i,row in Final_data.iterrows():\n",
        "    if row['overall'] >3:\n",
        "        sentiment.append(\"positive\")\n",
        "    elif row['overall'] <3:\n",
        "        sentiment.append(\"negative\")\n",
        "    else:\n",
        "        sentiment.append(\"neutral\")\n",
        "Final_data['sentiment']=sentiment\n",
        "Final_data['sentiment']= Final_data['sentiment'].map({'negative':0,'positive':2,'neutral':1})\n",
        "Final_data.drop('overall',axis=1)\n",
        "\n",
        "label=Final_data['sentiment'].values    #Targets\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fATRvzOfBc5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(data,label,test_size=0.10,random_state=42)\n",
        "\n",
        "tokenize=Tokenizer(num_words=1000)\n",
        "tokenize.fit_on_texts(X_train)\n",
        "\n",
        "X_train=tokenize.texts_to_matrix(X_train)\n",
        "X_test=tokenize.texts_to_matrix(X_test)\n",
        "\n",
        "vocab_size=len(tokenize.word_index) + 1\n",
        "\n",
        "#equalising the size of each sentence\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len=10\n",
        "\n",
        "X_train=pad_sequences(X_train,padding='post',maxlen=max_len)\n",
        "X_test=pad_sequences(X_test,padding='post',maxlen=max_len)\n",
        "Y_train=tf.one_hot(Y_train,depth=3)\n",
        "Y_test=tf.one_hot(Y_test,depth=3)\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_dim=10\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "     '/content/gdrive/My Drive/Amazon review/glove.6B.100d.txt',\n",
        "     tokenize.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjZKTRfrBQQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "b8c69418-43d2-4b6b-8462-c92c4ebadddb"
      },
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,Flatten,MaxPooling1D,Conv1D,Dropout\n",
        "\n",
        "#model\n",
        "\n",
        "model = Sequential([\n",
        "                    Embedding(input_dim=vocab_size,\n",
        "                              output_dim=embedding_dim,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=max_len,\n",
        "                              trainable=False),\n",
        "                    Dropout(0.5),\n",
        "                    Conv1D(16,kernel_size=3,activation='relu',padding='valid'),\n",
        "                    MaxPooling1D(),\n",
        "                    Conv1D(16,kernel_size=3,activation='relu',padding='valid'),\n",
        "                    MaxPooling1D(),\n",
        "                    \n",
        "                    \n",
        "                    Flatten(),\n",
        "                    Dense(256,activation='relu'),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(3,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 10, 10)            113190    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 10, 10)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 8, 16)             496       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 4, 16)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 2, 16)             784       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling (None, 1, 16)             0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               4352      \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 119,593\n",
            "Trainable params: 6,403\n",
            "Non-trainable params: 113,190\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ll4YeIjK_Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXTp0yowCqJf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bde0a87-abb8-459b-8fe8-4e865fd167c0"
      },
      "source": [
        "history= model.fit(X_train,Y_train,validation_data=(X_test,Y_test),batch_size=10,epochs=50,verbose=2\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3342/3342 - 15s - loss: 0.6711 - accuracy: 0.7887 - val_loss: 0.6374 - val_accuracy: 0.8002\n",
            "Epoch 2/50\n",
            "3342/3342 - 15s - loss: 0.6656 - accuracy: 0.7891 - val_loss: 0.6406 - val_accuracy: 0.8002\n",
            "Epoch 3/50\n",
            "3342/3342 - 14s - loss: 0.6648 - accuracy: 0.7890 - val_loss: 0.6384 - val_accuracy: 0.8002\n",
            "Epoch 4/50\n",
            "3342/3342 - 14s - loss: 0.6643 - accuracy: 0.7891 - val_loss: 0.6369 - val_accuracy: 0.8002\n",
            "Epoch 5/50\n",
            "3342/3342 - 14s - loss: 0.6644 - accuracy: 0.7891 - val_loss: 0.6374 - val_accuracy: 0.8002\n",
            "Epoch 6/50\n",
            "3342/3342 - 14s - loss: 0.6635 - accuracy: 0.7891 - val_loss: 0.6388 - val_accuracy: 0.8002\n",
            "Epoch 7/50\n",
            "3342/3342 - 14s - loss: 0.6635 - accuracy: 0.7890 - val_loss: 0.6389 - val_accuracy: 0.8002\n",
            "Epoch 8/50\n",
            "3342/3342 - 14s - loss: 0.6631 - accuracy: 0.7891 - val_loss: 0.6391 - val_accuracy: 0.8002\n",
            "Epoch 9/50\n",
            "3342/3342 - 14s - loss: 0.6629 - accuracy: 0.7890 - val_loss: 0.6373 - val_accuracy: 0.8002\n",
            "Epoch 10/50\n",
            "3342/3342 - 14s - loss: 0.6630 - accuracy: 0.7891 - val_loss: 0.6393 - val_accuracy: 0.8002\n",
            "Epoch 11/50\n",
            "3342/3342 - 14s - loss: 0.6624 - accuracy: 0.7891 - val_loss: 0.6380 - val_accuracy: 0.8002\n",
            "Epoch 12/50\n",
            "3342/3342 - 14s - loss: 0.6622 - accuracy: 0.7890 - val_loss: 0.6375 - val_accuracy: 0.8002\n",
            "Epoch 13/50\n",
            "3342/3342 - 14s - loss: 0.6622 - accuracy: 0.7890 - val_loss: 0.6420 - val_accuracy: 0.8002\n",
            "Epoch 14/50\n",
            "3342/3342 - 15s - loss: 0.6620 - accuracy: 0.7891 - val_loss: 0.6374 - val_accuracy: 0.8002\n",
            "Epoch 15/50\n",
            "3342/3342 - 15s - loss: 0.6619 - accuracy: 0.7890 - val_loss: 0.6390 - val_accuracy: 0.8002\n",
            "Epoch 16/50\n",
            "3342/3342 - 14s - loss: 0.6616 - accuracy: 0.7891 - val_loss: 0.6389 - val_accuracy: 0.8002\n",
            "Epoch 17/50\n",
            "3342/3342 - 14s - loss: 0.6615 - accuracy: 0.7890 - val_loss: 0.6380 - val_accuracy: 0.8002\n",
            "Epoch 18/50\n",
            "3342/3342 - 14s - loss: 0.6614 - accuracy: 0.7890 - val_loss: 0.6378 - val_accuracy: 0.8002\n",
            "Epoch 19/50\n",
            "3342/3342 - 14s - loss: 0.6615 - accuracy: 0.7890 - val_loss: 0.6379 - val_accuracy: 0.8002\n",
            "Epoch 20/50\n",
            "3342/3342 - 14s - loss: 0.6613 - accuracy: 0.7891 - val_loss: 0.6372 - val_accuracy: 0.8002\n",
            "Epoch 21/50\n",
            "3342/3342 - 14s - loss: 0.6612 - accuracy: 0.7890 - val_loss: 0.6374 - val_accuracy: 0.8002\n",
            "Epoch 22/50\n",
            "3342/3342 - 14s - loss: 0.6612 - accuracy: 0.7890 - val_loss: 0.6389 - val_accuracy: 0.8002\n",
            "Epoch 23/50\n",
            "3342/3342 - 15s - loss: 0.6611 - accuracy: 0.7891 - val_loss: 0.6376 - val_accuracy: 0.8002\n",
            "Epoch 24/50\n",
            "3342/3342 - 14s - loss: 0.6610 - accuracy: 0.7891 - val_loss: 0.6378 - val_accuracy: 0.8002\n",
            "Epoch 25/50\n",
            "3342/3342 - 14s - loss: 0.6612 - accuracy: 0.7890 - val_loss: 0.6379 - val_accuracy: 0.8002\n",
            "Epoch 26/50\n",
            "3342/3342 - 14s - loss: 0.6611 - accuracy: 0.7891 - val_loss: 0.6374 - val_accuracy: 0.8002\n",
            "Epoch 27/50\n",
            "3342/3342 - 14s - loss: 0.6620 - accuracy: 0.7891 - val_loss: 0.6376 - val_accuracy: 0.8002\n",
            "Epoch 28/50\n",
            "3342/3342 - 14s - loss: 0.6610 - accuracy: 0.7891 - val_loss: 0.6381 - val_accuracy: 0.8002\n",
            "Epoch 29/50\n",
            "3342/3342 - 14s - loss: 0.6614 - accuracy: 0.7890 - val_loss: 0.6378 - val_accuracy: 0.8002\n",
            "Epoch 30/50\n",
            "3342/3342 - 14s - loss: 0.6611 - accuracy: 0.7890 - val_loss: 0.6373 - val_accuracy: 0.8002\n",
            "Epoch 31/50\n",
            "3342/3342 - 14s - loss: 0.6612 - accuracy: 0.7891 - val_loss: 0.6381 - val_accuracy: 0.8002\n",
            "Epoch 32/50\n",
            "3342/3342 - 14s - loss: 0.6610 - accuracy: 0.7891 - val_loss: 0.6375 - val_accuracy: 0.8002\n",
            "Epoch 33/50\n",
            "3342/3342 - 14s - loss: 0.6610 - accuracy: 0.7891 - val_loss: 0.6396 - val_accuracy: 0.8002\n",
            "Epoch 34/50\n",
            "3342/3342 - 14s - loss: 0.6613 - accuracy: 0.7890 - val_loss: 0.6375 - val_accuracy: 0.8002\n",
            "Epoch 35/50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhTsWHZ6C5kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotting the curve for loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "#plotting the curve for accuracy\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Accuracy vs. epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2eWq2nYmfgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}